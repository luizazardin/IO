{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3 - Empirical IO\n",
    "## Luiza Zardin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from numpy import log as log\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Getting the data\n",
    "data = pd.read_csv('data_ps2.txt',delimiter='\\t',header=None)\n",
    "data.columns = ['car', 'year', 'firm', 'price', 'quantity', 'weight', 'hp', 'ac', 'nest3', 'nest4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Computing the HZ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logsumexp(X): # x is a 3-d matrix, len(x_grid) x len(lim_seq) x len([0, 1])\n",
    "    A = np.amax(X, axis=2)\n",
    "    return np.log(np.sum(np.exp(X - A[:,:,None]), axis=2)) + A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "data = pd.read_csv('rustdata1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate (analytically) the gradient of the log-likelihood function in Rust with respect to the parameters of the model and write down the analytic results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rust (1987) problem is: $$\\log\\mathcal{L} = \\sum_t^{T}\\log\\mathbb{P}\\left.\\Big[i_t \\,\\middle|\\, x_t\\Big]\\right.$$ where $\\theta = [\\theta_c, RC]$ and $\\theta_p$ is estimated outside of the ML estimation.\n",
    "The expected continuation value given today's mileage and replacement decision, $EV\\Big(x_t, i_t\\Big)$, can be found solving for a fixed point:\n",
    "$$EV\\Big(x_t, i_t\\Big) = \\int \\log\\Big(\\sum_{i'=0,1}\\exp\\Big(u(x',i') + \\beta \\cdot EV\\Big(x', i'\\Big)\\Big)\\Big) p(x'|\\theta_p, x_t, i_t)$$\n",
    "With $EV\\Big(x_t, i_t\\Big)$, I can find: $$\\tilde{V}\\Big(x_t, i_t\\Big)=\\left\\{\\begin{matrix}\n",
    "-c(0) -RC + \\beta EV\\Big(x_t, 1\\Big) &amp; \\text{if } i_t = 1 \\\\ \n",
    "-c(x_t) + \\beta EV\\Big(x_t, 0\\Big) &amp; \\text{if } i_t = 0\n",
    "\\end{matrix}\\right.$$ And using (4.13) from Rust (1987): $$\\mathbb{P}\\left.\\Big[i_t \\,\\middle|\\, x_t\\Big]\\right. = \\frac{\\exp\\Big(\\tilde{V}\\Big(x_t, i_t\\Big)\\Big)}{\\exp\\Big(\\tilde{V}\\Big(x_t, 0\\Big)\\Big)+\\exp\\Big(\\tilde{V}\\Big(x_t, 1\\Big)\\Big)}$$ This probability takes $\\theta_c$, $RC$, $\\beta$ and $\\theta_p$ as given.\n",
    "The derivatives are: $$\\frac{\\partial \\log\\mathbb{P}}{\\partial \\theta} = \\frac{1}{\\mathbb{P}}\\frac{\\partial \\mathbb{P}}{\\partial \\theta}$$$$\\frac{\\partial \\mathbb{P}}{\\partial \\theta}\\Big[x_t, i_t\\Big] = \\frac{\\exp\\Big(\\tilde{V}\\Big(x_t, 0\\Big)+\\tilde{V}\\Big(x_t, 1\\Big)\\Big)}{\\Big(\\exp\\Big(\\tilde{V}\\Big(x_t, 0\\Big)\\Big)+\\exp\\Big(\\tilde{V}\\Big(x_t, 1\\Big)\\Big)\\Big)^2}\\Big(\\frac{\\partial \\tilde{V}}{\\partial \\theta}\\Big(x_t, i_t\\Big)\\Big)-\\frac{\\partial \\tilde{V}}{\\partial \\theta}\\Big(x_t, 1-i_t\\Big)\\Big)\\Big)$$ We could use the implicit function theorem to get the derivative of $EV$, but this will lead to a functional equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Estimation MLE and MPEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model using the NPMLE approach of Rust. You will want to use the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the transition probabilities in a separate first stage. You should have 5 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deltamax = data['x_t'].diff().max()\n",
    "deltas = data['x_t'].diff()[data['x_t'].diff()>0]\n",
    "k = 5\n",
    "n = np.size(deltas,0)\n",
    "limit = np.linspace(0,np.ceil(deltamax),k+1)\n",
    "theta_3 = np.zeros((k,1))\n",
    "for i in range(k):\n",
    "    theta_3[i] =  np.sum((deltas>=limit[i])&(deltas<limit[i+1]))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute $EV (x;\\theta)$ for a given guess of the parameters via the fixed point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 1 # number of parameters in the following function\n",
    "def c(x, theta_1): # currently defined as a linear function, which apparently Rust preferred\n",
    "    return theta_1 * x\n",
    "\n",
    "# Define profit function\n",
    "def pi(x, i, theta_1, RC): # x is an array, i is not\n",
    "    if i == 0:\n",
    "        pi = -c(x, theta_1)\n",
    "    if i == 1:\n",
    "        pi = -(RC - c(np.zeros(x.shape), theta_1))\n",
    "    return pi\n",
    "\n",
    "# Compute EV(x, θ) via the fixed point\n",
    "l = 4 # how fine the discretized x grid is, l=1 is as fine as Δy from lim_seq, l=2 is twice as fine, etc.\n",
    "def EV_fp(theta_1, RC):\n",
    "    x_max = 14 # maximum value of x - in data, nothing above 13.8...\n",
    "    Delta_y = lim_seq[1] # Δy - the size of the increments in lim_seq\n",
    "    x = np.linspace(0, x_max, x_max/Delta_y*l+1) # need such that it includes the addition of Δy, which is characterized by lim_seq\n",
    "    maxx = np.amax(x) # maximum value in the discretized x grid\n",
    "    i = np.array([0, 1])\n",
    "    \n",
    "    # Construct the payoffs\n",
    "    Deltay = np.tile(lim_seq, (len(x), 1))\n",
    "    xplusDeltay = np.tile(x, (len(lim_seq), 1)).T + Deltay # matrix where each column is x + multiple of Δy\n",
    "    xplusDeltay = np.where(xplusDeltay <= maxx, xplusDeltay, maxx) # keep the highest values of x within the specified domain\n",
    "    idx = np.tile(np.arange(len(x)), (len(lim_seq), 1)).T + np.arange(len(lim_seq)) * l\n",
    "    idx = np.where(idx <= len(x) - 1, idx, len(x) - 1) # index used to differentially move up columns\n",
    "    pi0 = pi(xplusDeltay, 0, theta_1, RC)\n",
    "    pi1 = pi(xplusDeltay, 1, theta_1, RC)\n",
    "    \n",
    "    # Construct p(x_t+1 | i=1)\n",
    "    pr_i1 = np.vstack((np.ones(len(lim_seq)), np.zeros((len(x) - 1, len(lim_seq)))))\n",
    "    \n",
    "    # Initialize the loop\n",
    "    init_EV = np.zeros((len(x), len(i)))\n",
    "    EV_tau = init_EV\n",
    "    err = 1\n",
    "    err_tol = 1e-12\n",
    "    iter_num = 1\n",
    "    iter_lim = 5000\n",
    "    while err > err_tol and iter_num < iter_lim:\n",
    "        pdv0 = pi0 + beta * (np.tile(EV_tau[:, 0], (len(lim_seq), 1)).T)[idx, np.arange(len(lim_seq))]\n",
    "        pdv1 = pi1 + beta * (np.tile(EV_tau[:, 1], (len(lim_seq), 1)).T)[idx, np.arange(len(lim_seq))]\n",
    "        logsumexp_prepr = logsumexp(np.dstack((pdv0, pdv1)))\n",
    "        EV_tau1_i0 = np.sum(logsumexp_prepr * transition_pr, axis=1)\n",
    "        EV_tau1_i1 = np.sum(logsumexp_prepr * pr_i1, axis=1)\n",
    "        EV_tau1 = np.vstack((EV_tau1_i0, EV_tau1_i1)).T\n",
    "        err = np.amax(np.abs(EV_tau1 - EV_tau))\n",
    "        EV_tau = EV_tau1\n",
    "        iter_num += 1\n",
    "    if iter_num == iter_lim:\n",
    "        print(\"EV didn't converge.\")\n",
    "    return x, EV_tau\n",
    "\n",
    "# Define EV function\n",
    "def EV_fct(EV_fp, x_space, x, i): # x is an array, i is not\n",
    "    return np.interp(x, x_space, EV_fp[:,i]) # interpolate \n",
    "\n",
    "# Define choice-specific value function\n",
    "def v(x, i, theta_1, RC, EV, x_space): # x is an array, i is not\n",
    "    v = pi(x, i, theta_1, RC) + beta * EV_fct(EV, x_space, x, i)\n",
    "    return v\n",
    "\n",
    "# Construct CCP given EV(x, θ)\n",
    "def CCP(x, i, theta_1, RC):\n",
    "    x_space, EV = EV_fp(theta_1, RC)\n",
    "    V1 = v(x, 1, theta_1, RC, EV, x_space)\n",
    "    V0 = v(x, 0, theta_1, RC, EV, x_space)\n",
    "    Vi = V0*(i == 0) + V1*(i == 1)\n",
    "    pr = np.divide(np.exp(Vi), np.exp(V0) + np.exp(V1))\n",
    "    return pr\n",
    "\n",
    "# Construct the likelihood\n",
    "def loglikelihood(theta, x, i):\n",
    "    theta_1 = theta[:-1]\n",
    "    RC = theta[-1]\n",
    "    logl = np.sum(np.log(CCP(x, i, theta_1, RC)))\n",
    "    return -logl # negative because we are using a minimizer, but it's MLE\n",
    "\n",
    "# Construct likelihood's gradient\n",
    "def loglikelihood_grad(theta, x, i): # must accept same arguments as likelihood()\n",
    "    return 0 # need to compute this by hand (and write it in question 1)\n",
    "\n",
    "# Solve via MLE\n",
    "init_guess = np.zeros(q + 1)\n",
    "# res = minimize(loglikelihood, init_guess, args=(data['x_t'], data['d_t']), method='BFGS', jac=loglikelihood_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model using the MPEC method of Su and Judd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a = np.ones(3)\n",
    "print(a)\n",
    "b = np.zeros((5,3))\n",
    "print(np.vstack((a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results in a table, including the nonparametric answers below and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Stata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is taken from Han Hong’s problem set at Stanford, the idea is that we can use the arguments in Hotz-Miller (1993), or Pesendorfer Schmidt-Dengler (2008) to construct an optimization free method to recover the utility pa- rameters in the Rust problem.\n",
    "We begin by defining the choice specific value function with $\\varepsilon_{it}$ i.i.d. and EV.\n",
    "$$v(x,d) = u(x,d) + \\beta \\int \\log \\left( \\sum_{d' \\in D} \\exp(v(x', d')) \\right) p(x'|x, d) \\mathrm{d}x'$$$$v(x,d) = u(x,d) + \\beta \\int \\log \\left( \\sum_{d' \\in D} \\exp(v(x', d') - v(x', 1)) \\right) p(x'|x, d) \\mathrm{d}x' + \\beta \\int v(x,1)p(x'|x,1) \\mathrm{d}x'$$\n",
    "1. Estimate $p(x′|x,d)$ non parametrically or parametrically (for example as a set of multinomial with $n$ outcomes or an exponential distribution). Call your estimate $\\hat{p}(x′|x,d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Estimate $p(d|x)$ (the CCP) non-parametrically. You can use the binomial logit model with a basis function (increasing number of terms) or you can use a kernel such as ksdensity or ecdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating market shares\n",
    "data['share'] = data['quantity']/(1e8)\n",
    "\n",
    "#Constructing the BLP instruments\n",
    "for index, row in data.iterrows():\n",
    "    #mean of characteristics of competing firms in a giving year\n",
    "    ind = ((data['year'] == data.loc[index,'year']) & (data['firm']!=data.loc[index,'firm']))\n",
    "    data.loc[index,'z_compw'] = np.mean(data.loc[ind,'weight'])\n",
    "    data.loc[index,'z_comphp'] = np.mean(data.loc[ind,'hp'])\n",
    "    data.loc[index,'z_compac'] = np.mean(data.loc[ind,'ac'])\n",
    "    \n",
    "#For later renormalization of the coefficients\n",
    "factors = (-np.mean(data['price']),np.mean(data['weight']),np.mean(data['hp']))\n",
    "\n",
    "#Normalizing weight, hp and price\n",
    "data['weight'] = data['weight']/np.mean(data['weight'])\n",
    "data['hp'] = data['hp']/np.mean(data['hp'])\n",
    "data['price'] = data['price']/np.mean(data['price'])\n",
    "\n",
    "#Constructing the LHS variable for the regression\n",
    "for year in data.year.unique():\n",
    "    index = data['year']==year\n",
    "    data.loc[index,'share0'] = 1 - np.sum(data.loc[index ,'share'])\n",
    "\n",
    "data['y'] = log(data['share']) - log(data['share0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I estimate $\\hat{\\theta}$ using the following GMM procedure: \n",
    "$$\\max_\\theta Q_n\\big(\\theta \\big)$$ \n",
    "\n",
    "where $Q_n\\big(\\theta\\big) = -\\frac{1}{2}g_n\\big(\\theta\\big)'\\,\\hat{W}\\,g_n\\big(\\theta\\big)$ and \n",
    "\n",
    "$g_n\\big(\\theta\\big) = \\frac{1}{n}\\sum_{r=1}^R g\\big(w_r; \\theta\\big)$.\n",
    "\n",
    "$g\\big(w_r; \\theta\\big) = z_r \\cdot (y_r - x_r \\cdot \\theta)$ is the residual $\\xi_{jt}$ from the model above times the instruments calculated using $w_r$, a row of data.\n",
    "\n",
    "The efficient GMM is estimated with weighting matrix $\\hat{W}=\\left(\\frac{1}{n} \\sum_{o=1}^n{\\Big(g(w_o,\\theta_{first})-g_n(w,\\theta_{first})\\Big) \\cdot \\Big(g(w_o,\\theta_{first}) - g_n(w,\\theta_{first})\\Big)'}\\right)^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining GMM objects\n",
    "# we have n observations, m regressors and k instruments\n",
    "# y is a nx1 vectorm\n",
    "# x is a nxm matrix\n",
    "# z is a nxk matrix\n",
    "\n",
    "def gn(y,x,z,theta):\n",
    "    return z.T * (y - x * theta.reshape((x.shape[1],1)))\n",
    "     \n",
    "# Optimal GMM weighting matrix\n",
    "def Wmtrx(y,x,z,theta):\n",
    "    result = np.zeros((z.shape[1],z.shape[1]))\n",
    "    for i in range(y.shape[0]):\n",
    "        result += gn(y[i,:],x[i,:],z[i,:],theta) * gn(y[i,:],x[i,:],z[i,:],theta).T\n",
    "    result = 1/y.shape[0] * result\n",
    "    return np.matrix(result).I\n",
    "\n",
    "# GMM objective function (as a function of the parameters of the model, for given data)\n",
    "def Qn(theta,y,x,z,W):\n",
    "    return 1/2 * gn(y,x,z,theta).T * W * gn(y,x,z,theta)\n",
    "\n",
    "# GMM using optimal weighting matrix\n",
    "def GMM(y,x,z,guess):\n",
    "    W = np.eye(z.shape[1])\n",
    "    res = minimize(Qn,guess,args=(y,x,z,W),method='Nelder-Mead')\n",
    "    W = Wmtrx(y,x,z,res.x)\n",
    "    return minimize(Qn,res.x,args=(y,x,z,W),method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimating GMM for each year\n",
    "m = 4\n",
    "coefs = np.zeros((data.year.unique().shape[0],m))\n",
    "\n",
    "for i, year in enumerate(data.year.unique().tolist()):\n",
    "    \n",
    "    index = data['year']==year\n",
    "    \n",
    "    y = np.matrix(data.loc[index,'y']).T\n",
    "    x = np.matrix(data.loc[index,['price','weight','hp','ac']])\n",
    "    z = np.matrix(data.loc[index,['z_compw','z_comphp','z_compac','weight','hp','ac']])\n",
    "    \n",
    "    tguess = np.matrix(np.zeros([1,x.shape[1]]))\n",
    "    coefs[i,:] = GMM(y,x,z,tguess).x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price (-alpha)</th>\n",
       "      <th>weight</th>\n",
       "      <th>hp</th>\n",
       "      <th>ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>14.372416</td>\n",
       "      <td>0.063108</td>\n",
       "      <td>-19.703589</td>\n",
       "      <td>-3.023163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>9.205850</td>\n",
       "      <td>-1.854889</td>\n",
       "      <td>-14.487028</td>\n",
       "      <td>-1.100564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>6.932062</td>\n",
       "      <td>-2.469909</td>\n",
       "      <td>-12.324078</td>\n",
       "      <td>-0.489103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price (-alpha)    weight         hp        ac\n",
       "1990       14.372416  0.063108 -19.703589 -3.023163\n",
       "1991        9.205850 -1.854889 -14.487028 -1.100564\n",
       "1992        6.932062 -2.469909 -12.324078 -0.489103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing coefficients\n",
    "pd.DataFrame(data=coefs[:,:],index=data.year.unique(),columns=['price (-alpha)','weight','hp','ac']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain why own and cross price elasticites from this logit model may be unrealistic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the logit model, own price elasticities are increasing in price, which is somewhat unrealistic (we would think people who buy\n",
    "expensive products are less sensitive to price). Also, cross price elasticities depend only on market shares and prices but not on similarities between goods (IIA property), which is also unrealistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Logit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write down market share for a single product as a function of the vector $\\delta^*$ and the nesting parameter $\\sigma$. Use the $\\sigma$ and group notation used by Berry (1994 Rand), not the $\\lambda$ notation used by Train and McFadden.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that $s_{jt} = s_{jt|g} s_g$. Using the logit formula we get that:\n",
    "$$s_{jt} = \\frac{\\exp\\left(\\frac{\\delta_{jt}^*}{1 - \\sigma}\\right)}{\\left(\\sum_{k \\in \\mathcal{J}_g} \\exp\\left(\\frac{\\delta_{kt}^*}{1 - \\sigma}\\right)\\right)^{\\sigma} \\left(\\sum_h \\left(\\sum_{k \\in \\mathcal{J}_h} \\exp\\left( \\frac{\\delta_{kt}^*}{1 - \\sigma} \\right) \\right)^{1 - \\sigma}\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invert that equation to solve for $\\delta_j^*$ as a function of market shares, within group shares, and $\\sigma$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the outside good we have:\n",
    "$$s_{0t} = \\frac{1}{\\sum_g \\left(\\sum_{j \\in \\mathcal{J}_g} \\exp\\left( \\frac{\\delta_{jt}^*}{1 - \\sigma} \\right) \\right)^{1 - \\sigma}}.$$\n",
    "Taking log's and differences we get:\n",
    "$$\\log s_{jt} - \\log s_{0t} = \\frac{\\delta_{jt}^*}{1-\\sigma} - \\sigma \\log{D_{gt}}$$\n",
    "where $D_{gt}=\\sum_{j \\in g} e^{\\delta_{jt}^*/(1-\\sigma)}$. That yields:\n",
    "$$\\log s_{jt} - \\log s_{0t} = x'_{jt}\\beta_t - \\alpha_t p_{jt} + \\sigma \\log s_{jt|g} + \\xi_{jt}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate the demand system parameters using just the demand-side moment conditions for this nested-logit model. Instrument for price using the average of product characteristics (i.e. weight, hp, ac) of products produced by other firms in a given year. Similarly, construct the instrumental variables by using the average of product characteristics of other products (including those of the same firm) within the same group in a given year. For your nests, use two different nesting structures: one that groups products based on the “nest3” variable, and one that groups products based on the “nest4” variable. (Implicitly, the outside good is in its own additional group in each year.) Calculate 2SLS estimates, allowing all coefficients to vary across the three years of data, and use them as starting values for the optimization in your GMM routine. Report your results for $n = 3$ (inside) groups and $n = 4$ (inside) groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nestnms = ['nest3', 'nest4']\n",
    "sharesnms = ['share_n3', 'share_n4']\n",
    "instrnms = [['z_compw_n3','z_comphp_n3','z_compac_n3'],['z_compw_n4','z_comphp_n4','z_compac_n4']]\n",
    "\n",
    "for n in range(2):\n",
    "\n",
    "    #Constructing within group shares\n",
    "    for i, year in enumerate(data.year.unique().tolist()):\n",
    "        for j, nest in enumerate(data.eval(nestnms[n]).unique().tolist()):\n",
    "            index = ((data['year']==year) & (data[nestnms[n]]==nest))\n",
    "            qnt = sum(data.loc[index, 'quantity'])\n",
    "            data.loc[index, sharesnms[n]] = log(data.loc[index, 'quantity']/qnt)\n",
    "\n",
    "    #Constructing within group instruments\n",
    "    for index, row in data.iterrows():\n",
    "        #mean of characteristics of competing firms in a giving year\n",
    "        ind = ((data['year'] == data.loc[index,'year']) & (data[nestnms[n]] == data.loc[index,nestnms[n]])\n",
    "               & (data['car'] != data.loc[index,'car']))\n",
    "        data.loc[index,instrnms[n][0]] = np.mean(data.loc[ind,'weight'])\n",
    "        data.loc[index,instrnms[n][1]] = np.mean(data.loc[ind,'hp'])\n",
    "        data.loc[index,instrnms[n][2]] = np.mean(data.loc[ind,'ac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 2SLS and GMM estimates for the Nested Logit\n",
    "k = 0\n",
    "m = 5\n",
    "coefs = np.zeros((2*data.year.unique().shape[0],m))\n",
    "\n",
    "for i, year in enumerate(data.year.unique().tolist()):\n",
    "    for j, nest in enumerate(nestnms):\n",
    "        index = data['year']==year\n",
    "        y = np.matrix(data.loc[index, 'y']).T\n",
    "        x = np.matrix(data.loc[index, ['price','weight','hp','ac',sharesnms[j]]])\n",
    "        z = np.matrix(data.loc[index, instrnms[j] + ['z_compw','z_comphp','z_compac','weight','hp','ac']])\n",
    "        t2SLS = (x.T * z * (z.T * z).I * z.T * x).I * x.T * z * (z.T * z).I * z.T * y\n",
    "        coefs[k,:] = GMM(y,x,z,t2SLS).x\n",
    "        k += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price (-alpha)</th>\n",
       "      <th>weight</th>\n",
       "      <th>hp</th>\n",
       "      <th>ac</th>\n",
       "      <th>sjtg (sigma)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990,nest3</th>\n",
       "      <td>-2.407367</td>\n",
       "      <td>-1.818427</td>\n",
       "      <td>0.416030</td>\n",
       "      <td>1.827852</td>\n",
       "      <td>1.104780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990,nest4</th>\n",
       "      <td>1.929430</td>\n",
       "      <td>-0.956090</td>\n",
       "      <td>-4.849609</td>\n",
       "      <td>0.503870</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991,nest3</th>\n",
       "      <td>0.125390</td>\n",
       "      <td>-2.017619</td>\n",
       "      <td>-1.362030</td>\n",
       "      <td>0.502705</td>\n",
       "      <td>1.041927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991,nest4</th>\n",
       "      <td>2.641000</td>\n",
       "      <td>0.166342</td>\n",
       "      <td>-6.703076</td>\n",
       "      <td>-0.053670</td>\n",
       "      <td>0.930611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992,nest3</th>\n",
       "      <td>0.604341</td>\n",
       "      <td>-2.153633</td>\n",
       "      <td>-1.792808</td>\n",
       "      <td>0.250344</td>\n",
       "      <td>0.982752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992,nest4</th>\n",
       "      <td>2.510011</td>\n",
       "      <td>-0.202878</td>\n",
       "      <td>-6.484872</td>\n",
       "      <td>-0.172786</td>\n",
       "      <td>0.849344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            price (-alpha)    weight        hp        ac  sjtg (sigma)\n",
       "1990,nest3       -2.407367 -1.818427  0.416030  1.827852      1.104780\n",
       "1990,nest4        1.929430 -0.956090 -4.849609  0.503870      0.950700\n",
       "1991,nest3        0.125390 -2.017619 -1.362030  0.502705      1.041927\n",
       "1991,nest4        2.641000  0.166342 -6.703076 -0.053670      0.930611\n",
       "1992,nest3        0.604341 -2.153633 -1.792808  0.250344      0.982752\n",
       "1992,nest4        2.510011 -0.202878 -6.484872 -0.172786      0.849344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0\n",
    "labels = []\n",
    "for i, year in enumerate(data.year.unique().tolist()):\n",
    "    for j, nest in enumerate(nestnms):\n",
    "        labels.append(str(year)+','+nest)\n",
    "        \n",
    "pd.DataFrame(data=coefs[:,:],index=labels,columns=['price (-alpha)','weight','hp','ac','sjtg (sigma)']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Suppose instead that we estimated a version of a nested-logit model that pooled all three years’ worth of data. What assumption on intertemporal substitution patterns is implicit in this choice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assume that the conditional error is independent over time. In practice, because we have unobserved quality components in the errors and this are very likely to be persistent, this assumption would be incorrect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Are your estimates of sigma sensitive to the number of groups? Can you give an explanation for this result?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, $\\sigma$ is usually higher with three groups than with four groups. $\\sigma$ captures the within group correlation in market shares. I would expect to have higher within group correlation the finer the nest structure, but maybe the groups in the nests with 4 groups are not very well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) How does your estimate of $\\alpha$ change across the three years' of data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ varies a lot over the years (even from negative to positive) and it appears to be increasing over the years for a given nest structure. With four nests we get higher $\\alpha$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) How is nested logit an improvement over plain logit?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nested logit allows for a more flexible substitution pattern. The IIA property now holds only for products within the same group and not across all goods. Within each group we have standard logit, but products in different nests have less in common, and therefore are not as good substitutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Think of another nesting structure you would use, and explain what additional data you would need to estimate it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use multiple levels of nesting or potentially overlapping nests, but this would require a modification of the equations used before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Why might all forms of nested logit be problematic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback with nested-logit is that we need to a-priori classify the products, so if the groups are not well defined we'll have the same propblems as in the logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose we were interested in improving the substitution patterns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Would the Multinomial Probit model be appealing in this setting? Why, or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probit replaces the assumption that the errors are EV and i.i.d. with the assumption that the errors are normally distributed but with an arbitrary covariance matrix. This allows for very flexible substitution patterns but creates a huge number of parameters to estimate in the covariance matrix. Usually to make it operational, one has to make assumptions in the covariance matrix, which are as arbitraty as choosing nests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Would the Pure Characteristics model be appealing in this setting? Why, or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure characteristics model assumes that there is no unobserved product specific characteristics, so it could not account for a BWM having a higher market share than other cars with similar observed characteristics and lower price. It would not be appealing in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now assume that the model is a logit model but each individual has a different price coefficient, i.e.\n",
    "$$u_{ijt} = x_{jt}\\beta - \\alpha_i p_{jt} + \\xi_{jt} + \\varepsilon_{ijt}.$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Suppose $\\alpha_i = 1 / y_i$ and $y_i$ is distributed lognormally. Write out the moment conditions and estimation algorithm you would use to estimate this model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that $\\alpha_i = 1 / y_i$, and $\\log(y_i) \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)$. Note that if $Y = 1 / X$, then $\\log(Y) = \\log(X^{-1}) = -\\log(X)$, so if $\\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $\\mathbb{E}[Y] = \\exp(-\\mu + \\frac12 \\sigma^2)$ by the formula for the expectation of a log normally-distributed random variable. We can therefore derive the following mean utility:\n",
    "$$\\delta_{jt}^* = x_{jt}\\beta - \\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) p_{jt} + \\xi_{jt}.$$\n",
    "Note that because $\\varepsilon_{ijt} \\overset{iid}{\\sim} EV(I)$, we can write the logit shares as follows:\n",
    "$$\\begin{align}\n",
    "s_{jt} &= \\int_0^\\infty \\frac{\\exp\\left(x_{jt} \\beta - \\frac{1}{y_i} p_{jt} + \\xi_{jt} \\right)}{1 + \\sum_k \\exp\\left(x_{kt} \\beta - \\frac{1}{y_i} p_{kt} + \\xi_{kt} \\right)} \\mathrm{d}F(y_i) \\\\\n",
    "&= \\int_0^\\infty \\frac{\\exp\\left(x_{jt} \\beta - \\frac{1}{y_i} p_{jt} + \\xi_{jt} \\right)}{1 + \\sum_k \\exp\\left(x_{kt} \\beta - \\frac{1}{y_i} p_{kt} + \\xi_{kt} \\right)} \\frac{1}{y_i \\sigma_y \\sqrt{2 \\pi}} \\exp\\left( -\\frac{(\\log(y_i) - \\mu_y)^2}{2\\sigma_y^2} \\right) \\mathrm{d}y_i \\\\\n",
    "&= \\int_0^\\infty \\frac{\\exp\\left(\\delta_{jt}^* + \\left(\\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) - \\frac{1}{y_i}\\right) p_{jt} \\right)}{1 + \\sum_k \\exp\\left(\\delta_{kt}^* + \\left(\\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) - \\frac{1}{y_i}\\right) p_{kt} \\right)} \\frac{1}{y_i \\sigma_y \\sqrt{2 \\pi}} \\exp\\left( -\\frac{(\\log(y_i) - \\mu_y)^2}{2\\sigma_y^2} \\right) \\mathrm{d}y_i \\\\\n",
    "&= \\int_{-\\infty}^\\infty \\frac{\\exp\\left(\\delta_{jt}^* + \\left(\\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) - \\exp(-\\psi_i)\\right) p_{jt} \\right)}{1 + \\sum_k \\exp\\left(\\delta_{kt}^* + \\left(\\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) - \\exp(-\\psi_i)\\right) p_{kt} \\right)} \\frac{1}{\\sigma_y \\sqrt{2 \\pi}} \\exp\\left( -\\frac{(\\psi_i - \\mu_y)^2}{2\\sigma_y^2} \\right) \\mathrm{d}\\psi_i,\n",
    "\\end{align}$$\n",
    "where the final line follows from setting $\\psi_i := \\log(y_i)$. This gives us the following moment condition (assuming we are not pooling across years):\n",
    "$$\\mathbb{E}[g(w_{lt}, \\theta_0)] = \\mathbb{E}\\left[z_{lt} \\left(\\delta_{lt}^* - x_{lt}\\beta + \\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) p_{lt} \\right)\\right] = 0,$$\n",
    "where $z_{lt}$ are our instruments for price. This moment condition allows us to use GMM. We do not observe mean utility, however; therefore, we propose a system based on BLP. Let\n",
    "$$g_n(\\theta) = \\frac{1}{n} \\sum_{l=1}^n \\left[ z_{l}' \\left(\\delta_{l}^* - x_{l}\\beta + \\exp\\left(-\\mu_y + \\frac12 \\sigma_y^2\\right) p_l \\right) \\right]$$\n",
    "\n",
    "where $\\theta=\\Big[\\beta, \\mu_y, \\sigma_y\\Big]$.\n",
    "\n",
    "Use the following algorithm to obtain an estimate $\\hat{\\theta}$. (*Note: Time subscripts have been dropped, but if we wish to estimate individual years, we can repeat the following algorithm for the data separated by year.*)\n",
    "\n",
    "**Step 1:** Take an initial value of $\\hat{\\theta} \\in \\Theta$ and an initial weighting matrix (e.g. identity matrix).\n",
    "\n",
    "**Step 2:** Use the following contraction mapping proposed by BLP to solve for $\\delta$ such that $s_{j}(\\delta, \\theta) = \\tilde{s}_{j}$ (i.e. that observed shares and predicted shares are identical):\n",
    "$$\\delta^{(k)}(\\theta) = \\delta^{(k-1)}(\\theta) + \\log(\\tilde{s}_j) - \\log(s_j(\\delta^{(k-1)}, \\theta)).$$\n",
    "Iterate on this equation using beginning with an initial guess of $\\delta$ (make sure it's reasonable--perhaps start with one of the previous $\\hat{\\delta}$ values) until $\\left\\vert \\delta^{(k)}(\\theta) - \\delta^{(k-1)}(\\theta)\\right\\vert < 10^{-13}$. To compute the numerical integral, we can use a Gauss-Hermite approximation (which is very fast) but we need to give it a sufficiently large number of points so that it is accurate and precise. (*Note: We are assuming here that the contraction mapping holds in this case. BLP proved that it holds in the normally distributed $\\beta$ case, so it might not hold in this inverse log-normally distributed $\\alpha$ case.*)\n",
    "\n",
    "**Step 3:** Evaluate objective function: $-\\frac12 g_n(\\theta)' \\hat{W}_n g_n(\\theta)$.\n",
    "\n",
    "**Step 4:** Choose next $\\hat{\\theta}' \\in \\Theta$ in systematic way to search over $\\Theta$ to maximize objective function. Return to Step 2 with $\\hat{\\theta}'$. Repeat Steps 2 - 4 until convergence (i.e., we have reached the maximum).\n",
    "\n",
    "**Step 5:** Update the weighting matrix with the estimated $\\hat{\\theta}$ that minimizes the objective function and repeat Steps 1 through 4 (2-stage efficient GMM).\n",
    "\n",
    "This returns an estimate $\\hat{\\theta}_{BLP}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Now suppose $\\alpha_i = \\alpha_1 + \\alpha_2 / y_i$ and $y_i$ is still distributed lognormally. How exactly would this change the estimation? Write out the moment conditions and estimation algorithm you would use to estimate $\\beta$ in this model. Are the parameters of the lognormal distribution, $\\alpha_1$ and $\\alpha_2$ identified by the data provided to you?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha_i = \\alpha_1+\\alpha_2/y_i \\implies \\mu_\\alpha = \\alpha_1+\\alpha_2 \\cdot \\exp(-\\mu_{y}+\\frac{1}{2}\\sigma_{y}^{2})$ hence $\\alpha_1$ is not identified, $\\alpha_2$ might be by the variance or higher moments. The moment conditions would be derived by plugging the new $\\mu_\\alpha$ into the integral above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Among the other parameters you would have estimated in question 9a are the mean and the variance of the lognormal distribution. Now assume that you knew that the mean of income was \\$35,000 and that the standard deviation is \\$45,000. Using only the demand system, estimate the $\\beta$ parameters under these assumptions. Continue to use the moment conditions involving the excluded instruments you used for plain logit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fixed parameters on the distribution of $y_i$ there are only 3 parameters to estimate. $$35000 = \\exp(\\mu_y + \\frac{\\sigma_y^2}{2})$$$$45000^2 = (\\exp(\\sigma_y^2) - 1)\\cdot\\exp(2\\mu_y+\\sigma_y^2)$$ Hence: $$2(\\log 35000 - \\mu_y) = \\sigma_y^2$$$$2\\log45000 = \\log(\\exp(\\sigma_y^2) - 1) + 2\\mu_y+\\sigma_y^2$$ And: $$\\mu_y = \\log\\Big(\\frac{35000^2}{\\sqrt{45000^2 + 35000^2}}\\Big)$$$$\\sigma_y = \\sqrt{\\log\\Big(1+\\frac{45000^2}{35000^2}\\Big)}$$\n",
    "\n",
    "In order to estimate $\\beta$, we need to be able to evaluate $\\tilde{s}_j$. We will use Gauss-Hermite quadrature because it is quick to compute with a large number of weights to ensure precision and accuracy. Note:\n",
    "$$\\begin{align}\n",
    "s_j &= \\int_{-\\infty}^\\infty \\frac{1}{\\sigma_y \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\psi_i - \\mu_y)^2}{2\\sigma_y^2}\\right) h(\\psi_i) \\mathrm{d}\\psi_i \\\\\n",
    "&= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{\\pi}} \\exp(-x_i^2) h(\\sqrt{2}\\sigma_y x_i + \\mu_y) \\mathrm{d}x_i \\qquad \\text{where } x_i = \\frac{\\psi_i - \\mu_i}{\\sqrt{2}\\sigma_y} \\\\\n",
    "&\\approx \\frac{1}{\\sqrt{\\pi}} \\sum_i \\omega_i h(\\sqrt{2}\\sigma_y x_i + \\mu_y)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculates shares by using Gauss-Hermite quadrature\n",
    "def sj(delta, p, m, theta):\n",
    "    mu_y = theta[0]\n",
    "    sigma_y = theta[1]\n",
    "    P = np.tile(p, (m,1))\n",
    "    Delta = np.tile(delta, (m,1))\n",
    "    n = len(p)\n",
    "    points, weights = np.polynomial.hermite.hermgauss(m)\n",
    "    points = np.tile(points, (n,1)).T\n",
    "    weights = np.tile(weights, (n,1)).T\n",
    "    psi = np.sqrt(2) * sigma_y * points + mu_y\n",
    "    h_num = np.exp(Delta + np.multiply((np.exp(-mu_y + 1/2 * sigma_y ** 2) - np.exp(-psi)), P))\n",
    "    h_denom = np.tile(1 + np.sum(h_num, axis=1), (n,1)).T\n",
    "    h = np.divide(h_num, h_denom)\n",
    "    s = 1 / np.sqrt(np.pi) * np.sum(np.multiply(weights, h), axis=0)\n",
    "    return s\n",
    "\n",
    "#Calculates the deltas\n",
    "def delta_fp(init_delta, shares, p, theta):\n",
    "    error = 1\n",
    "    iteration = 0\n",
    "    delta = init_delta\n",
    "    while error > 1e-13 and iteration < 1e3:\n",
    "        delta_old = delta\n",
    "        delta = delta_old + np.log(shares) - np.log(sj(delta_old, p, 30, theta))\n",
    "        iteration += 1\n",
    "        error = np.amax(np.absolute(delta - delta_old))\n",
    "    if error > 1e-13:\n",
    "        print(\"Fixed point did not converged\")\n",
    "        print(\"Iterations: \" + str(iteration))\n",
    "        print(\"Error: \" + str(error))\n",
    "    return delta\n",
    "\n",
    "mu_y = np.log(35000 ** 2 / (45000**2 + 35000**2) ** (1/2))\n",
    "sigma_y = np.log(1 + 45000 ** 2 / 35000 ** 2) ** (1/2)\n",
    "theta = [mu_y, sigma_y]\n",
    "\n",
    "m = 3\n",
    "coefs = np.zeros((data.year.unique().shape[0],m))\n",
    "\n",
    "for i, year in enumerate(data.year.unique().tolist()):\n",
    "    \n",
    "    index = data['year']==year\n",
    "    #calculating delta\n",
    "    delta_BLP = delta_fp(np.zeros(len(data.loc[index, 'share'])), data.loc[index, 'share'], data.loc[index, 'price'], theta)\n",
    "    y = np.matrix(delta_BLP + np.exp(-mu_y + 1/2 * sigma_y ** 2) * data.loc[index, 'price']).reshape(delta_BLP.shape[0],1)\n",
    "    x = np.matrix(data.loc[index, ['weight','hp','ac']])\n",
    "    z = np.matrix(data.loc[index, ['z_compw','z_comphp','z_compac','weight','hp','ac']])\n",
    "    #GMM estimation\n",
    "    init_guess = np.zeros(3)\n",
    "    coefs[i,:] = GMM(y,x,z,init_guess).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>hp</th>\n",
       "      <th>ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>-7.379300</td>\n",
       "      <td>-1.388828</td>\n",
       "      <td>2.570353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>-7.069365</td>\n",
       "      <td>-1.164322</td>\n",
       "      <td>1.229033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>-7.105062</td>\n",
       "      <td>-0.915393</td>\n",
       "      <td>0.968160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        weight        hp        ac\n",
       "1990 -7.379300 -1.388828  2.570353\n",
       "1991 -7.069365 -1.164322  1.229033\n",
       "1992 -7.105062 -0.915393  0.968160"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=coefs[:,:],index=data.year.unique(),columns=['weight','hp','ac']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)  Are the own and cross price elasticities from this system more realistic than those in the plain and nested logit models, and if so why? One way to evaluate this is to produce the $J$ matrix of diversion ratios, as well as reporting the median own price elasticity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mixed-logit model imposes less a-priori restrictions on the substitution patterns, mainly because it does not exhibit logit's restrictive independence of irrelevant alternatives (IIA) property. We should get more realistic own and cross price elasticities, as long as the assumption of log-normality is not too crazy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
